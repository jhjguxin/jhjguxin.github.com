<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: cache | Francis's Octopress Blog]]></title>
  <link href="http://jhjguxin.github.io/tags/cache/atom.xml" rel="self"/>
  <link href="http://jhjguxin.github.io/"/>
  <updated>2013-11-12T19:23:30+08:00</updated>
  <id>http://jhjguxin.github.io/</id>
  <author>
    <name><![CDATA[Francis Jiang]]></name>
    <email><![CDATA[864248765@qq.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Advanced Caching: Part 1 - Caching Strategies]]></title>
    <link href="http://jhjguxin.github.io/blog/2012/08/29/advanced-caching-part-1-caching-strategies/"/>
    <updated>2012-08-29T23:15:00+08:00</updated>
    <id>http://jhjguxin.github.io/blog/2012/08/29/advanced-caching-part-1-caching-strategies</id>
    <content type="html"><![CDATA[<h2>Advanced Caching: Part 1 &ndash; Caching Strategies</h2>

<p>First, let&rsquo;s start with a brief overview of the different types of caching. We&rsquo;ll start from 50,000ft and work our way down.</p>

<p>HTTP Caching: Uses HTTP headers (Last-Modified, ETag, If-Modified-Since, If-None-Match, Cache-Control) to determine if the browser can use a locally stored version of the response or if it needs to request a fresh copy from the origin server. Rails makes it easy to use HTTP caching, however the cache is managed outside your application. You may have notice the config.cache_control and Rack::Cache, Rack::ETag, Rack::ConditionalGet middlewares. These are used for HTTP caching.</p>

<p>Page Caching: PRAISE THE GODS if you actually can use page caching in your application. Page caching is the holy grail. Save the entire thing. Don&rsquo;t hit the stack &amp; give some prerendered stuff back. Great for worthless applications without authentication and other highly dynamic aspects. This essentially works like HTTP caching, but the response will always contain the entire page. With page caching the application is skipping the work.</p>

<p>Action Caching: Essentially the same as page caching, except all the before filters are run allowing you to check authentication and other stuff that may have prevented the request form rendering.</p>

<p>Fragment Caching: Store parts of views in the cache. Usually for caching partials or large bits of HTML that are independent from other parts. IE, a list of top stories or something like that.</p>

<p>Rails.cache: All cached content except cached pages are stored in the Rails.cache. We&rsquo;ll use this fact that later. You can cache arbitrary content in the Rails cache. You may cache a large complicated query that you don&rsquo;t want to wait to reinstantiate a ton of ActiveRecord::Base objects.</p>

<p>Under the Hood
All the caching layers are built on top of the next one. Page caching and HTTP caching are different because they do not use Rails.cache The cache is essentially a key-value store. Different things can be persisted. Strings are most common (for HTML fragments). More complicated objects can be persisted as well. Let&rsquo;s go through some examples of manually using the cache to store things. I am using memcached with dalli for all these examples. Dalli is the default memcached driver.</p>

<h1>Rails.cache.write takes two values: key and a value</h1>

<blockquote><p>Rails.cache.write &lsquo;foo&rsquo;, &lsquo;bar&rsquo;
=> true</p></blockquote>

<h1>We can read an object back</h1>

<blockquote><p>Rails.cache.read &lsquo;foo&rsquo;
=> &ldquo;bar&rdquo;</p></blockquote>

<h1>We can store a complicated object as well</h1>

<blockquote><p>hash = { :this => { :is => &lsquo;a hash&rsquo; }}
Rails.cache.write &lsquo;complicated-object&rsquo;, object
Rails.cache.read &lsquo;complicated-object&rsquo;
=> {:this=>{:is=>&ldquo;a hash&rdquo;}}</p></blockquote>

<h1>If we want something that doesn&rsquo;t exist, we get nil</h1>

<blockquote><p>Rails.cache.read &lsquo;we-havent-cached-this-yet&rsquo;
=> nil</p></blockquote>

<h1>&ldquo;Fetch&rdquo; is the most common pattern. You give it a key and a block</h1>

<h1>to execute to store if the cache misses. The blocks&rsquo;s return value is</h1>

<h1>then written to the cache. The block is not executed if there is a</h1>

<h1>hit.</h1>

<blockquote><p>Rails.cache.fetch &lsquo;huge-array&rsquo; do</p>

<pre><code>huge_array = Array.new
1000000.times { |i| huge_array &lt;&lt; i }
huge_array # retrun value is stored in cache
</code></pre>

<p>  end
=> [huge array] # took some time to generate
Rails.cache.read &lsquo;huge-array&rsquo;
=> [huge array] # but returned instantly</p></blockquote>

<h1>You can also delete everything from the cache</h1>

<blockquote><p>Rails.cache.clear
=> [true]
Those are the basics of interacting with the Rails cache. The rails cache is a wrapper around whatever functionality is provided by the underlying storage system. Now we are ready to move up a layer.</p></blockquote>

<p>Understanding Fragment Caching
Fragment caching is taking rendered HTML fragments and storing them in the cache. Rails provides a cache view helper for this. Its most basic form takes no arguments besides a block. Whatever is rendered during the block will be written back to the cache. The basic principle behind fragment caching is that it takes much less time fetch pre-rendered HTML from the cache, then it takes to generate a fresh copy. This is appallingly true. If you haven&rsquo;t noticed, view generation can be very costly. If you have cachable content and are not using fragment caching then you need to implement this right away! Let&rsquo;s say you have generated a basic scaffold for a post:</p>

<p>$ rails g scaffold post title:string content:text author:string
Let&rsquo;s start with the most common use case: caching information specific to one thing. IE: One post. Here is a show view:</p>

<!-- nothing fancy going on here -->


<p><p>
  <b>Title:</b>
  &lt;%= @post.title %>
</p></p>

<p><p>
  <b>Content:</b>
  &lt;%= @post.content %>
</p>
Let&rsquo;s say we wanted to cache fragment. Simply wrap it in cache and Rails will do it.</p>

<p>&lt;%= cache &ldquo;post-#{@post.id}&rdquo; do %>
  <p></p>

<pre><code>&lt;b&gt;Title:&lt;/b&gt;
&lt;%= @post.title %&gt;
</code></pre>

<p>  </p></p>

<p>  <p></p>

<pre><code>&lt;b&gt;Content:&lt;/b&gt;
&lt;%= @post.content %&gt;
</code></pre>

<p>  </p>
&lt;% end %>
The first argument is the key for this fragment. The rendered HTML is stored with this key: views/posts-1. Wait what? Where did that &lsquo;views&rsquo; come from? The cache view helper automatically prepends &lsquo;views&rsquo; to all keys. This is important later. When you first load the page you&rsquo;ll see this in the log:</p>

<p>Exist fragment? views/post-2 (1.6ms)
Write fragment views/post-2 (0.9ms)
You can see the key and the operations. Rails is checking to see if the specific key exists. It will fetch or write it. In this case, it has not been stored so it is written. When you reload the page, you&rsquo;ll see a cache hit:</p>

<p>Exist fragment? views/post-2 (0.6ms)
Read fragment views/post-2 (0.0ms)
There we go. We got HTML from the cache instead of rendering it. Look at the response times for the two requests:</p>

<p>Completed 200 OK in 17ms (Views: 11.6ms | ActiveRecord: 0.1ms)
Completed 200 OK in 16ms (Views: 9.7ms | ActiveRecord: 0.1ms)
Very small differences in this case. 2ms different in view generation. This is a very simple example, but it can make a world of difference in more complicated situations.</p>

<p>You are probably asking the question: &ldquo;What happens when the post changes?&rdquo; This is an excellent question! What well if the post changes, the cached content will not be correct. It is up to us to remove stuff from the cache or figure out a way to get new content from the cache. Let&rsquo;s assume that our blog posts now have comments. What happens when a comment is created? How can handle this?</p>

<p>This is a very simple problem. What if we could figure out a solution to this problem: How can we create a cache miss when the associated object changes? We&rsquo;ve already demonstrated how we can explicitly set a cache key. What if we made a key that&rsquo;s dependent on the time the object was last updated? We can create a key composed of the record&rsquo;s ID and its updated_at timestamp! This way the cache key will change as the content changes and we will not have to expire things manually. (We&rsquo;ll come back to sweepers later). Let&rsquo;s change our cache key to this:</p>

<p>&lt;% cache &ldquo;post-#{@post.id}&rdquo;, @post.updated_at.to_i do %>
Now we can see we have a new cache key that&rsquo;s dependent on the object&rsquo;s timestamp. Check out the rails log:</p>

<p>Exist fragment? views/post-2/1304291241 (0.5ms)
Write fragment views/post-2/1304291241 (0.4ms)
Cool! Now let&rsquo;s make it so creating a comment updates the post&rsquo;s timestamp:</p>

<p>class Comment &lt; ActiveRecord::Base
  belongs_to :post, :touch => true
end
Now all comments will touch the post and change the updated_at timestamp. You can see this in action by touch'ing a post.</p>

<p>Post.find(1).touch</p>

<p>Exist fragment? views/post-2/1304292445 (0.4ms)
Write fragment views/post-2/1304292445 (0.4ms)
This concept is known as: auto expiring cache keys. You create a composite key with the normal key and a time stamp. This will create some memory build up as objects are updated and no longer fresh. Here&rsquo;s an example. You have that fragment. It is cached. Then someone updates the post. You now have two versions of the fragment cached. If there are 10 updates, then there are 10 different versions. Luckily for you, this is not a problem for memcached! Memcached uses a LRU replacement policy. LRU stands for Least Recently Used. That means the key that hasn&rsquo;t been requested in the longest time will be replaced by newer content when needed. For example, assume your cache can only hold 10 posts. The next update will create a new key and hence new content. Version 0 will be deleted and version 11 will be stored in the cache. The total amount of memory is cycled between things that are requested. There are two things to consider in this approach. 1: You will not be able to ensure that content is kept in the cache as long as possible. 2. You will never have to worry about expiring things manually as long as timestamps are updated in the model layer. I&rsquo;ve found it is orders of magnitude easier to add a few :touch => true&rsquo;s to my relationships than it is to maintain sweepers. More on sweepers later. We must continue exploring cache keys.</p>

<p>Rails uses auto-expiring cache keys by default. The problem is they are not mentioned at all the documentation or in the guides. There is one very handy method: ActiveRecord::Base.cache_key. This will generate a key like this: posts/2-20110501232725. This is the exact same thing we did ourselves. This method is very important because depending on what type of arguments you pass into the cache method, a different key is generated. For the time being, this code is functionally equal to our previous examples.</p>

<p>&lt;%= cache @post do %>
The cache helper takes different forms for arguments. Here are some examples:</p>

<p>cache &lsquo;explicit-key&rsquo;      # views/explicit-key
cache @post               # views/posts/2-1283479827349
cache [@post, &lsquo;sidebar&rsquo;]  # views/posts/2-2348719328478/sidebar
cache [@post, @comment]   # views/posts/2-2384193284878/comments/1-2384971487
cache :hash => :of_things # views/localhost:3000/posts/2?hash_of_things
If an Array is the first arguments, Rails will use cache key expansion to generate a string key. This means calling doing logic on each object then joining each result together with a &lsquo;/&rsquo;. Essentially, if the object responds to cache_key, it will use that. Else it will do various things. Here&rsquo;s the source for expand_cache_key:</p>

<p>def self.expand_cache_key(key, namespace = nil)
  expanded_cache_key = namespace ? &ldquo;#{namespace}/&rdquo; : &ldquo;&rdquo;</p>

<p>  prefix = ENV[&ldquo;RAILS_CACHE_ID&rdquo;] || ENV[&ldquo;RAILS_APP_VERSION&rdquo;]
  if prefix</p>

<pre><code>expanded_cache_key &lt;&lt; "#{prefix}/"
</code></pre>

<p>  end</p>

<p>  expanded_cache_key &lt;&lt;</p>

<pre><code>if key.respond_to?(:cache_key)
  key.cache_key
elsif key.is_a?(Array)
  if key.size &gt; 1
    key.collect { |element| expand_cache_key(element) }.to_param
  else
    key.first.to_param
  end
elsif key
  key.to_param
end.to_s
</code></pre>

<p>  expanded_cache_key
end
This is where all the magic happens. Our simple fragment caching example could easily be converted into an idea like this: The post hasn&rsquo;t changed, so cache the entire result of /posts/1. You can do with this action caching or page caching.</p>

<p>Moving on to Action Caching
Action caching is an around filter for specific controller actions. It is different from page caching since before filters are run and may prevent access to certain pages. For example, you may only want to cache if the user is logged in. If the user is not logged in they should be redirected to the log in page. This is different than page caching. Page caching bypasses the rails stack completely. Most web applications of legitimate complexity cannot use page caching. Action caching is the next logical step for most web applications. Let&rsquo;s break the idea down: If the post hasn&rsquo;t changed, return the entire cached page as the HTTP response, else render the show view, cache it, and return that as the HTTP response. Or in code:</p>

<h1>Note: you cannot run this code! This is just an example of what&rsquo;s</h1>

<h1>happening under the covers using concepts we&rsquo;ve already covered.</h1>

<p>Rails.cache.fetch &lsquo;views/localhost:3000/posts/1&rsquo; do
  @post = Post.find params[:id]
  render :show
end
Declaring action caching is easy. Here&rsquo;s how you can cache the show action:</p>

<p>class PostsController &lt; ApplicationController</p>

<p>  caches_action :show</p>

<p>  def show</p>

<pre><code># do stuff
</code></pre>

<p>  end
end
Now refresh the page and look at what&rsquo;s been cached.</p>

<p>Started GET &ldquo;/posts/2&rdquo; for 127.0.0.1 at 2011-05-01 16:54:43 -0700
  Processing by PostsController#show as HTML
  Parameters: {&ldquo;id&rdquo;=>&ldquo;2&rdquo;}
Read fragment views/localhost:3000/posts/2 (0.5ms)
Rendered posts/show.html.erb within layouts/application (6.1ms)
Write fragment views/localhost:3000/posts/2 (0.5ms)
Completed 200 OK in 16ms (Views: 8.6ms | ActiveRecord: 0.1ms)
Now that the show action for post #2 is cached, refresh the page and see what happens.</p>

<p>Started GET &ldquo;/posts/2&rdquo; for 127.0.0.1 at 2011-05-01 16:55:27 -0700
  Processing by PostsController#show as HTML
  Parameters: {&ldquo;id&rdquo;=>&ldquo;2&rdquo;}
Read fragment views/localhost:3000/posts/2 (0.6ms)
Completed 200 OK in 1ms
Damn. 16ms vs 1ms. You can see the difference! You can also see Rails reading that cache key. The cache key is generated from the url with action caching. Action caching is a combination of a before and around filter. The around filter is used to capture the output and the before filter is used to check to see if it&rsquo;s been cached. It works like this:</p>

<p>Execute before filter to check to see if cache key exists?
Key exists? &ndash; Read from cache and return HTTP Response. This triggers a render and prevents any further code from being executed.
No key? &ndash; Call all controller and view code. Cache output using Rails.cache and return HTTP response.
Now you are probably asking the same question as before: &ldquo;What do we do when the post changes?&rdquo; We do the same thing as before: we create a composite key with a string and a time stamp. The question now is, how do we generate a special key using action caching?</p>

<p>Action caching generates a key from the current url. You can pass extra options using the :cache_path option. Whatever is in this value is passed into url_for using the current parameters. Remember in the view cache key examples what happened when we passed in a hash? We get a much different key than before:</p>

<p>views/localhost:3000/posts/2?hash_of_things
Rails generated a URL based key instead of the standard views key. This is because you may different servers. This ensures that each server has it&rsquo;s own cache key. IE, server one does not collide with server two. We could generate our own url for this resource by doing something like this:</p>

<p>url_for(@post, :tag => @post.updated_at.to_i)
This will generate this url:</p>

<p><a href="http://localhost:3000/posts/1?tag=234897123978">http://localhost:3000/posts/1?tag=234897123978</a>
Notice the ?tag=23481329847. This is a hack that aims to stop browsers from using HTTP caching on specific urls. If the URL has changed (timestamp changes) then the browser knows it must request a fresh copy. Rails 2 used to do this for assets like CSS and JS. Things have changed with the asset pipeline.</p>

<p>Here&rsquo;s an example of generating a proper auto expring key for use with action caching.</p>

<p>caches_action :show, :cache_path => proc { |c|
  # c is the instance of the controller. Since action caching
  # is declared at the class level, we don&rsquo;t have access to instance
  # variables. If cache_path is a proc, it will be evaluated in the
  # the context of the current controller. This is the same idea
  # as validations with the :if and :unless options
  #
  # Remember, what is returned from this block will be passed in as
  # extra parameters to the url_for method.
  post = Post.find c.params[:id]
  { :tag => post.updated_at.to_i }
end
This calls url_for with the parameters already assigned by it through the router and whatever is returned by the block. Now if you refresh the page, you&rsquo;ll have this:</p>

<p>Started GET &ldquo;/posts/2&rdquo; for 127.0.0.1 at 2011-05-01 17:11:22 -0700
  Processing by PostsController#show as HTML
  Parameters: {&ldquo;id&rdquo;=>&ldquo;2&rdquo;}
Read fragment views/localhost:3000/posts/2?tag=1304292445 (0.5ms)
Rendered posts/show.html.erb within layouts/application (1.7ms)
Write fragment views/localhost:3000/posts/2?tag=1304292445 (0.5ms)
Completed 200 OK in 16ms (Views: 4.4ms | ActiveRecord: 0.1ms)
And volia! Now we have an expiring cache key for our post! Let&rsquo;s dig a little deeper. We know the key. Let&rsquo;s look into the cache and see what it actually is! You can see the key from the log. Look it up in the cache.</p>

<blockquote><p>Rails.cache.read &lsquo;views/localhost:3000/posts/2?tag=1304292445&rsquo;
=> &ldquo;&lt;!DOCTYPE html>\n<html>\n<head>&hellip;..&rdquo;
It&rsquo;s just a straight HTML string. Easy to use and return as the body. This method works well for singular resources. How can we handle the index action? I&rsquo;ve created 10,000 posts. It takes a good amount of time to render that page on my computer. It takes over 10 seconds. The question is, how can we cache this? We could use the most recently updated post for the time stamp. That way, when one post is updated, it will move to the top and create a new cache key. Here is the code without any action caching:</p></blockquote>

<p>Started GET &ldquo;/posts&rdquo; for 127.0.0.1 at 2011-05-01 17:18:11 -0700
  Processing by PostsController#index as HTML
  Post Load (54.1ms)  SELECT &ldquo;posts&rdquo;.* FROM &ldquo;posts&rdquo; ORDER BY updated_at DESC LIMIT 1
Read fragment views/localhost:3000/posts?tag=1304292445 (1.5ms)
Rendered posts/index.html.erb within layouts/application (9532.3ms)
Write fragment views/localhost:3000/posts?tag=1304292445 (36.7ms)
Completed 200 OK in 10088ms (Views: 9535.6ms | ActiveRecord: 276.2ms)
Now with action caching:</p>

<p>Started GET &ldquo;/posts&rdquo; for 127.0.0.1 at 2011-05-01 17:20:47 -0700
  Processing by PostsController#index as HTML
Read fragment views/localhost:3000/posts?tag=1304295632 (1.0ms)
Completed 200 OK in 11ms
Here&rsquo;s the code for action caching:</p>

<p>caches_action :index, :cache_path => proc {|c|
  { :tag => Post.maximum(&lsquo;updated_at&rsquo;) }
}
We&rsquo;ll come back to this situation later. This is a better way to do this. Points to the reader if they know the problem.</p>

<p>These are simple examples designed to show you who can create auto expiring keys for different situations. At this point we have not had to expire any thing ourselves! The keys have done it all for us. However, there are some times when you want more precise control over how things exist in the cache. Enter Sweepers.</p>

<p>Sweepers
Sweepers are HTTP request dependent observers. They are loaded into controllers and observe models the same way standard observers do. However there is one very important different. They are only used during HTTP requests. This means if you have things being created outside the context of HTTP requests sweepers will do you no good. For example, say you have a background process running that syncs with an external system. Creating a new model will not make it to any sweeper. So, if you have anything cached. It is up to you to expire it. Everything I&rsquo;ve demonstrated so far can be done with sweepers.</p>

<p>Each cache<em>* method has an opposite expire</em>* method. Here&rsquo;s the mapping:</p>

<p>caches_page , expire_page
caches_action , expire_action
cache , expire_fragment
Their arguments work the same with using cache key expansion to find a key to read or delete. Depending on the complexity of your application, it may be easy to use sweepers or it may be impossible. It&rsquo;s easy to use sweepers with these examples. We only need to tie into the save event. For example, when a update or delete happens we need to expire the cache for that specific post. When a create, update, or delete happens we need to expire the index action. Here&rsquo;s what the sweeper would look like:</p>

<p>class PostSweeper &lt; ActionController::Caching::Sweeper
  observe Post</p>

<p>  def after_create(post)</p>

<pre><code>expire_action :index
expire_action :show, :id =&gt; post
# this is the same as the previous line
expire_action :controller =&gt; :posts, :action =&gt; :show, :id =&gt; @post.id
</code></pre>

<p>  end
end</p>

<h1>then in the controller, load the sweeper</h1>

<p>class PostsController &lt; ApplicationController
  cache_sweeper :post_sweeper
end
I will not go into much depth on sweepers because they are the only thing covered in the rails caching guide. The work, but I feel they are clumsy for complex applications. Let&rsquo;s say you have comments for posts. What do you do when a comment is created for a post? Well, you have to either create a comment sweeper or load the post sweeper into the comments controller. You can do either. However, depending on the complexity of your model layer, it may quickly become infeasible to do cache expiration with sweepers. For example, let say you have a Customer. A customer has 15 different types of associated things. Do you want to put the sweeper into 15 different controllers? You can, but you may forget to at some point.</p>

<p>The real problem with sweepers is that they cannot be used once your application works outside of HTTP requests. They can also be clumsy. I personally feel it&rsquo;s much easier to create auto expiring cache keys and only uses sweepers when I want to tie into very specific events. I&rsquo;d also argue that any well designed system does not need sweepers (or at least in very minimally).</p>

<p>Now you should have a good grasp on how the Rails caching methods work. We&rsquo;ve covered how fragment caching uses the current view to generate a cache key. We introduced the concept of auto expiring cache keys using ActiveRecord#cache_key to automatically expire cached content. We introduced action caching and how it uses url_for to generate a cache key. Then we covered how you can pass things into url_for to generate a time stamped key to expire actions automatically. Now that we understand these lower levels we can move up to page caching and HTTP caching.</p>

<p>Page Caching
Page caching bypasses the entire application by serving up a file in /public from disk. It is different from action or fragment caching for a two reasons: content is not stored in memory and content is stored directly on the disk. You use page caching the same way you use action caching. This means you can use sweepers and and all the other things associated with them. Here&rsquo;s how it works.</p>

<p>Webserver accepts an incoming request: GET /posts
File exists: /public/posts.html
posts.html is returned
Your application code is never called.
Since pages are written like public assets they are served as such. You will expliclity have to expire them. Warning! Forgetting to expire pages will cause you greif because you application code will not be called. Here&rsquo;s an example of page caching:</p>

<p>PostsController &lt; ApplicationController
  caches_page :index</p>

<p>  def index</p>

<pre><code># do stuff
</code></pre>

<p>  end
When the server receives a request to GET /posts it will write the response from the application to /public/posts.html. The .html part is the format for that request. For example you can use page caching with JSON. GET /posts.json would generate /public/posts.json.</p>

<p>Page caching is basically poor man&rsquo;s HTTP caching without any real benefits. HTTP caching is more useful.</p>

<p>I&rsquo;ve not covered page caching in much depth because it&rsquo;s very likely that if you&rsquo;re reading this page caching is not applicable to your application. The Rails guides cover page caching in decent fashion. Follow up there if you need more information.</p>

<p>HTTP Caching
HTTP caching is the most complex and powerful caching strategy you can use. With great power comes great responsiblity. HTTP caching works at the protocol level. You can configure HTTP caching so the browser doesn&rsquo;t even need to contact your server at all. There are many ways HTTP caching can be configured. I will not cover them all here. I will give you an overview on how the system works and cover some common use cases.</p>

<p>How It Works
HTTP caching works at the protocol level. It uses a combination of headers and response codes to indicate weather the user agent should make a request or use a locally stored copy instead. The invalidation or expiring is based on ETags and Last-Modified timestamps. ETag stands for &ldquo;entity tag&rdquo;. It&rsquo;s a unique fingerprint for this request. It&rsquo;s usually a checksum of the respnose body. Origin servers (computers sending the source content) can set either of these fields along with a Cache-Control header. The Cache-Control header tells the user agent what it can do with this response. It answers questions like: how long can I cache this for and am I allowed to cache it? When the user agent needs to make a request again it sends the ETag and/or the Last-Modified date to the origin server. The origin server decides based on the ETag and/or Last-Modified date if the user agent can use the cached copy or if it should use new content. If the server says use the cached content it will return status 304: Not Modified (aka fresh). If not it should return a 200 (cache is stale) and the new content which can be cached.</p>

<p>Let&rsquo;s use curl to see how this works out:</p>

<p>$ curl -I <a href="http://www.example.com">http://www.example.com</a>
HTTP/1.1 200 OK
Cache-Control: max-age=0, private, must-revalidate
Content-length: 822
Content-Type: text/html
Date: Mon, 09 Jul 2012 22:46:29 GMT
Last-Modified: Mon, 09 Jul 2012 21:22:11 GMT
Status: 200 OK
Vary: Accept-Encoding
Connection: keep-alive
The Cache-Control header is a tricky thing. There are many many ways it can be configured. Here&rsquo;s the two easiest ways to break it down: private means only the final user agent can store the response. Public means any server can cache this content. (You know requests may go through many proxies right?). You can specify an age or TTL. This is how long it can be cached for. Then there is another common situation: Don&rsquo;t check with the server or do check with the server. This particular Cache-Control header means: this is a private (think per user cache) and check with the server everytime before using it.</p>

<p>We can trigger a cache hit by sending the apporiate headers with the next request. This response only has a Last-Modified date. We can send this date for the server to compare. Send this value in the If-Modified-Since header. If the content hasn&rsquo;t changed since that date the server should return a 304. Here&rsquo;s an example using curl:</p>

<p>$ curl -I -H &ldquo;If-Modified-Since: Mon, 09 Jul 2012 21:22:11 GMT&rdquo; <a href="http://www.example.com">http://www.example.com</a>
HTTP/1.1 304 Not Modified
Cache-Control: max-age=0, private, must-revalidate
Date: Mon, 09 Jul 2012 22:55:53 GMT
Status: 304 Not Modified
Connection: keep-alive
This response has no body. It simply tells the user agent to use the locally stored version. We could change the date and get a different response.</p>

<p>$ curl -I -H &ldquo;If-Modified-Since: Sun, 08 Jul 2012 21:22:11 GMT&rdquo; <a href="http://www.example.com">http://www.example.com</a>
HTTP/1.1 200 OK
Cache-Control: max-age=0, private, must-revalidate
Content-length: 822
Content-Type: text/html
Date: Mon, 09 Jul 2012 22:57:19 GMT
Last-Modified: Mon, 09 Jul 2012 21:22:11 GMT
Status: 200 OK
Vary: Accept-Encoding
Connection: keep-alive
Caches determine freshness based on the If-None-Match and/or If-Modified-Since date. Using our existing 304 response we can supply a random etag to trigger a cache miss:</p>

<p>$ curl -I -H &lsquo;If-None-Match: &ldquo;foo&rdquo;&rsquo; -H &ldquo;If-Modified-Since: Mon, 09 Jul 2012 21:22:11 GMT&rdquo; <a href="http://www.example.com">http://www.example.com</a>
HTTP/1.1 304 Not Modified
Cache-Control: max-age=0, private, must-revalidate
Date: Mon, 09 Jul 2012 22:55:53 GMT
Status: 304 Not Modified
Connection: keep-alive
Etags are sent using the If-None-Match header. Now that we understand the basics we can move onto higher level discussion.</p>

<p>Rack::Cache
HTTP caching is implemented in the webserver itself or at the application level. It is implemented at the application level in Rails. Rack::Cache is a middleware that sits at the top of the stack and intercepts requests. It will pass requests down to your app and store their contents. Or will it call down to your app and see what ETag and/or timestamps it returns for validation purposes. Rack::Cache acts as a proxy cache. This means it must respect caching rules described in the Cache-Control headers coming out of your app. This means it cannot cache private content but it can cache public content. Cachable content is stored in memcached. Rails configures this automatically.</p>

<p>I&rsquo;ll cover one use case to illustrate how code flows through middleware stack to the actual app code and back up. Let&rsquo;s use a private per user cache example. Here&rsquo;s the cache control header: max-age-0, private, must-revalidate. Pretend this is some JSON API.</p>

<p>The client sends initial request to /api/tweets.json
Rack::Cache sees the request and ignores it since there is no caching information along with it.
Application code is called. It returns a 200 response with a date and the some Cache-Control header.
The client makes another request to /api/tweets.json with an If-Modified-Since header matching the date from the previous request.
Rack::Cache sees that his request has cache information associated with it. It checks to see how it should handle this request. According to the Cache-Control header it has expired and needs to be checked to see if it&rsquo;s ok to use. Rack::Cache calls the application code.
Application returns a response with the same date.
Rack::Cache recieves the response, compares the dates and determines that it&rsquo;s a hit. Rack::Cache sends a 304 back.
The client uses response body from request in step 1.
HTTP Caching in Rails
Rails makes it easy to implement HTTP caching inside your controllers. Rails provides two methods: stale? and fresh_when. They both do the same thing but in opposite ways. I prefer to use stale? because it makes more sense to me. stale? reminds more of Rails.cache.fetch so I stick with that. stale? works like this: checks to see if the incoming request ETag and/or Last-Modified date matches. If they match it calls head :not_modified. If not it can call a black of code to render a response. Here is an example:</p>

<p>def show
  @post = Post.find params[:id]
  stale? @post do</p>

<pre><code>respond_with @post
</code></pre>

<p>  end
end
Using stale? with an ActiveRecord object will automatically set the ETag and Last-Modified headers. The Etag is set to a MD5 hash of the objects cache_key method. The Last-Modified date is set to the object&rsquo;s updated_at method. The Cache-Control header is set to max-age=0, private, must-revalidate by default. All these values can be changed by passing in options to stale? or fresh_when. The methods take three options: :etag, :last_modified, and :public. Here are some more examples:</p>

<h1>allow proxy caches to store this result</h1>

<p>stale? @post, :public => true do
  respond_with @post
end</p>

<h1>Let&rsquo;s stay your posts are frozen and have no modifications</h1>

<p>stale? @post, :etag => @post.posted_at do
  respond_with @post
end
Now you should understand how HTTTP caching works. Here are the important bits of code inside Rails showing it all works.</p>

<h1>File actionpack/lib/action_controller/metal/conditional_get.rb, line 39</h1>

<p>def fresh_when(record_or_options, additional_options = {})
  if record_or_options.is_a? Hash</p>

<pre><code>options = record_or_options
options.assert_valid_keys(:etag, :last_modified, :public)
</code></pre>

<p>  else</p>

<pre><code>record  = record_or_options
options = { :etag =&gt; record, :last_modified =&gt; record.try(:updated_at) }.merge(additional_options)
</code></pre>

<p>  end</p>

<p>  response.etag          = options[:etag]          if options[:etag]
  response.last_modified = options[:last_modified] if options[:last_modified]
  response.cache_control[:public] = true if options[:public]</p>

<p>  head :not_modified if request.fresh?(response)
end
Here is the code for fresh?. This code should help you if you are confused on how resquests are validated. I found this code much easier to understand than the official spec.</p>

<p>def fresh?(response)
  last_modified = if_modified_since
  etag          = if_none_match</p>

<p>  return false unless last_modified || etag</p>

<p>  success = true
  success &amp;&amp;= not_modified?(response.last_modified) if last_modified
  success &amp;&amp;= etag_matches?(response.etag) if etag
  success
end</p>

<p><h2>Index</h2>
<ol></p>

<pre><code>&lt;li&gt;&lt;a href="http://www.broadcastingadam.com/2012/07/advanced_caching_part_1-caching_strategies"&gt;Caching Strategies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.broadcastingadam.com/2012/07/advanced_caching_part_2-using_strategies"&gt;Using Strategies Effectively&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.broadcastingadam.com/2012/07/advanced_caching_part_3-static_assets"&gt;Handling Static Assets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.broadcastingadam.com/2012/07/advanced_caching_part_4-stepping_outside_the_http_request"&gt;Stepping Outside the HTTP Request&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.broadcastingadam.com/2012/07/advanced_caching_part_5-tag_based_caching"&gt;Tag Based Caching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.broadcastingadam.com/2012/07/advanced_caching_part_6-fast_json_apis"&gt;Fast JSON APIs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.broadcastingadam.com/2012/07/advanced_caching_part_7-tips_and_tricks"&gt;Tips and Tricks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.broadcastingadam.com/2012/07/advanced_caching_part_8-conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
</code></pre>

<p></ol>
<h3>Contact Me</h3>
Find a problem or have a question about this post? <a href="http://twitter.com/adman65/">@adman65</a> on Twitter or Adman65 on #freenode. Find me in (#rubyonrails or #sproutcore). You can find my code on <a href="http://github.com/twinturbo/">GitHub</a> or hit me up on <a href="https://plus.google.com/u/0/116377228668850173159">Google+</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rails counter cache]]></title>
    <link href="http://jhjguxin.github.io/blog/2012/08/27/rails-counter-cache/"/>
    <updated>2012-08-27T00:44:00+08:00</updated>
    <id>http://jhjguxin.github.io/blog/2012/08/27/rails-counter-cache</id>
    <content type="html"><![CDATA[<h2>Rails counter cache</h2>

<p>这次就是讲用_count字段来缓存has_many的计数 </p>

<p>看Project和Task的例子:</p>

<div id="">
<pre><h1>Projects</h1>

<table>
<% for project in @projects %>
  <tr>
    <td><%= link_to project.name, poject_path(project) %></td>
    <td><small>(<%= pluralize project.tasks.size, 'task' %>)</small></td>
  </tr>
<% end %>
</table>
</pre>
</div>


<pre>上面的页面代码对所有的@projects显示tasks.size，看下log:</pre>


<div id="">
<pre>SQL (0.006385)  SELECT count(*) AS count_all FROM tasks WHERE (tasks.project_id = 326)
SQL (0.000220)  SELECT count(*) AS count_all FROM tasks WHERE (tasks.project_id = 327)
SQL (0.000383)  SELECT count(*) AS count_all FROM tasks WHERE (tasks.project_id = 328)
SQL (0.000197)  SELECT count(*) AS count_all FROM tasks WHERE (tasks.project_id = 329)
SQL (0.000215)  SELECT count(*) AS count_all FROM tasks WHERE (tasks.project_id = 330)</pre>
</div>


<pre>上面显示了对每个project都使用SQL来count tasks，我们采用eager loading看看能否改进性能:</pre>


<div id="">
<pre>class ProjectsController &lt; ApplicationController
  def index
    @projects = Project.find(:all, :include =&gt; :tasks)
  end
end</pre>
</div>


<pre>再来看看log:</pre>


<div id="">
<pre>Project Lood Incluing Associations (0.000954)  SELECT projects.'id' AS t0_r0, projects.'name' AS t0_r1, tasks.'id'
AS t1_r0, tasks.'name' AS t1_r1, tasks.'project_id' AS t1_r2 FROM projects LEFT OUTER JOIN tasks ON tasks.project
_id = projects.id</pre>
</div>


<pre>我们看到，使用eager loading确实只用一条SQL语句就完成工作，但是缺点是把tasks表所有的字段信息都取出来了，很多信息是 
没有用的。 

我们来看看更好的解决方案:</pre>


<div id="">
<pre>ruby script/generate migration add_tasks_count</pre>
</div>


<pre>我们新建一个migration，给projects表添加一个叫tasks_count的列:</pre>


<div id="">
<pre>class AddTasksCount &lt; ActiveRecord::Migration
  def self.up
    add_column :projects, :tasks_count, :integer, :default =&gt; 0

    Project.reset_column_information
    Project.find(:all).each do |p|
      p.update_attribute :tasks_count, p.tasks.length
    end
  end

  def self.down
    remove_column :projects, :tasks_count
  end
end</pre>
</div>


<pre>我们还需要告诉Task类开启counter cache:</pre>


<div id="">
<div>
<pre>class Task &lt; ActiveRecord::Base
  belongs_to :projects, :counter_cache =&gt; true
end</pre>
</div>
</div>


<pre>好了，我们把ProjectsController的index方法改回lazy loading，刷新页面，再看看log:</pre>


<div id="">
<pre>Project Lood (0.000295)  SELECT * FROM projects</pre>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[nginx缓存cache的5种方案]]></title>
    <link href="http://jhjguxin.github.io/blog/2011/11/30/nginxhuan-cun-cachede-5chong-fang-an/"/>
    <updated>2011-11-30T17:54:00+08:00</updated>
    <id>http://jhjguxin.github.io/blog/2011/11/30/nginxhuan-cun-cachede-5chong-fang-an</id>
    <content type="html"><![CDATA[<h2>nginx缓存cache的5种方案</h2>

<div>

1. 客户端浏览器上的缓存(非Cookie, Cookie中的内容为: 键和值均为string类型的键值对)

我们可以通过在Http回应中增加特定的头部说明来指定浏览器的缓存策略; 添加头部说明的手段既可以通过页面指令声明设置, 也可以通过编程方式设置.

&nbsp;

对于图片、Javascript脚本、CSS等资源, 可以在IIS管理器中, 右击图片等资源, 选择”属性” --&gt; HttpHeaders后, 勾选Enable Content Expiration并设置时间即可. 一种值得推荐的手段是, 将需要缓存的资源分类, 如: image/dynamic/、image/static/, 这样我们可以再文件夹上, 选择属性中的HttpHeaders进行设置. 否则, 针对每一个静态资源设置HttpHeaders将是件非常痛苦的事情. 此外, 还可以采用一款名为CacheRight的工具可以提供缓存资源的统一配置.

&nbsp;

<strong>查看或设置浏览器缓存位置:</strong>  IE --&gt; Internet选项 --&gt; 常规 --&gt; 临时Internet文件 --&gt; 设置

<strong> </strong>

<strong>Html</strong><strong>文件的Head</strong><strong>中的缓存设置:</strong>

&lt;meta http-equiv="pragma" content="no-cache" /&gt;

&lt;meta http-equiv="Cache-Control" content="no-cache" /&gt;

&lt;meta http-equiv="expires" content="Wed, 26 Feb 1997 08:21:57 GMT" /&gt;

<strong>浏览器中关于Cache</strong><strong>的3</strong><strong>属性:</strong>

Cache-Control:

设置相对过期时间, max-age指明以秒为单位的缓存时间. 若对静态资源只缓存一次, 可以设置max-age的值为315360000000 (一万年).

<strong>Http</strong><strong>协议的cache-control</strong><strong>的常见取值及其组合释义:</strong>

no-cache: 数据内容不能被缓存, 每次请求都重新访问服务器, 若有max-age, 则缓存期间不访问服务器.

no-store: 不仅不能缓存, 连暂存也不可以(即: 临时文件夹中不能暂存该资源)

private(默认): 只能在浏览器中缓存, 只有在第一次请求的时候才访问服务器, 若有max-age, 则缓存期间不访问服务器.

public: 可以被任何缓存区缓存, 如: 浏览器、服务器、代理服务器等

max-age: 相对过期时间, 即以秒为单位的缓存时间.

no-cache, private: 打开新窗口时候重新访问服务器, 若设置max-age, 则缓存期间不访问服务器.

private, 正数的max-age: 后退时候不会访问服务器

no-cache, 正数的max-age: 后退时会访问服务器

点击刷新: 无论如何都会访问服务器.

Expires:

设置以分钟为单位的绝对过期时间, 优先级比Cache-Control低, 同时设置Expires和Cache-Control则后者生效.

Last-Modified:

该资源的最后修改时间, 在浏览器下一次请求资源时, 浏览器将先发送一个请求到服务器上, 并附上If-Unmodified-Since头来说明浏览器所缓存资源的最后修改时间, 如果服务器发现没有修改, 则直接返回304(Not Modified)回应信息给浏览器(内容很少), 如果服务器对比时间发现修改了, 则照常返回所请求的资源.

&nbsp;

注意:

Last-Modified属性通常和Expires或Cache-Control属性配合使用, 因为即使浏览器设置缓存, 当用户点击”刷新”按钮时, 浏览器会忽略缓存继续向服务器发送请求, 这时Last-Modified将能够很好的减小回应开销.

&nbsp;

ETag将返回给浏览器一个资源ID, 如果有了新版本则正常发送并附上新ID, 否则返回304， 但是在服务器集群情况下, 每个服务器将返回不同的ID, 因此不建议使用ETag.

&nbsp;

以上描述的客户端浏览器缓存是指存储位置在客户端浏览器, 但是对客户端浏览器缓存的实际设置工作是在服务器上的资源中完成的. 虽然刚才我们介绍了有关于客户端浏览器缓存的属性, 但是实际上对这些属性的设置工作都需要在服务器的资源中做设置. 我们有两种操作手段对浏览器缓存进行设置, 一个是通过页面指令声明来设置, 另外一个是通过编程方式来设置.

&nbsp;

浏览器缓存的设置手段:

第一: 通过页面指令声明来设置HTTP的缓存

页面指令&lt;%@ OutputCache Location=”Any” Duration=”10” VaryByParam=”ProductId” VaryByHeader=”Accept-Language”%&gt;中的Location用来设置缓存的位置, 该属性常见的值为:

Any(默认): 输出缓存可以位于任何地点, 对应于HttpCacheability.Public. 如: 客户端浏览器、代理服务器或服务器本身.

Client: 只能位于发出请求的客户端浏览器, 对应于HttpCacheability.Private.

Downstream: 输出缓存可以位于除服务器本身的其他任何地方, 如: 客户端浏览器、代理服务器.

Server: 输出缓存位于Web服务器本身, 对应于HttpCacheability.Server

ServerAndClient: 输出缓存只能位于服务器本身或客户端浏览器, 对应于HttpCacheability.Private和HttpCacheability.Server

None: 禁用输出缓存, 对应于HttpCacheability.NoCache.

VaryByParam属性: 根据请求参数的不同而缓存不同的版本. 多个值用分号(;)分隔, *号表示为任意参数或参数组合缓存不同版本, “none”表示只缓存一个版本.

VaryByHeader属性: 根据请求头来缓存不同的版本, 如同一页面的不同语言版本.

VaryByCustom属性: 根据自定义参数来缓存不同的版本, 如: VaryByCunstom=”browser”是系统已实现的, 根据浏览器名称和版本号缓存不同的版本. 也可以, 根据自定义参数来缓存, 如: VaryByCustom=”happy”, 此时系统不知道如何解释happy, 因此需要在Global.asax或IHttpModule实现类中重写GetVaryByCustomString()方法, 来完成处理逻辑.

VaryByControl属性: 根据用户控件中的服务器控件ID来缓存不同版本.

更高级的方式, 是通过配置文件来设置HTTP的缓存.

页面指令为&lt;%@ OutputCache CacheProfile=”cacheconfig”%&gt;, 其中cacheconfig是配置文件中的缓存配置节中CacheProfile的名称.
<div>View Code
<div id="cnblogs_code_open_c6451953-d048-4c1a-aa1d-39b4e297c264">
<div>&lt;system.web&gt;
&lt;caching&gt;
&lt;outputCacheSettings&gt;
&lt;outputCacheProfiles&gt;
&lt;add name="cacheconfig" duration="10" varyByParam="none"/&gt;
&lt;/outputCacheProfiles&gt;
&lt;/outputCacheSettings&gt;
&lt;/caching&gt;
&lt;/system.web&gt;</div>
<div>
<h1>nginx缓存cache的5种方案</h1>
1、传统缓存之一（404）

这个办法是把nginx的404错误定向到后端，然后用proxy_store把后端返回的页面保存。

配置：

location / {
root /home/html/;#主目录
expires 1d;#网页的过期时间
error_page 404 =200 /fetch$request_uri;#404定向到/fetch目录下
}

location /fetch/ {#404定向到这里
internal;#指明这个目录不能在外部直接访问到
expires 1d;#网页的过期时间
alias /home/html/;#虚拟目录文件系统地址要和locaion /一致，proxy_store会将文件保存到这目录下
proxy_pass ;#后端upstream地址，/fetch同时是一个代理
proxy_set_header Accept-Encoding '';#让后端不要返回压缩（gzip或deflate）的内容，保存压缩后的内容会引发乱子。
proxy_store on;#指定nginx将代理返回的文件保存
proxy_temp_path /home/tmp;#临时目录，这个目录要和/home/html在同一个硬盘分区内
}

使用的时候还有要注意是nginx要有权限往/home/tmp和/home/html下有写入文件的权限，在linux下nginx一般会配置成nobody用户运行，这样这两个目录就要chown nobody，设成nobody用户专用，当然也可以chmod 777，不过所有有经验的系统管理员都会建议不要随便使用777。

2、传统缓存之二（!-e）

原理和404跳转基本一致，但更简洁一些：

location / {
root /home/html/;
proxy_store on;
proxy_set_header Accept-Encoding '';
proxy_temp_path /home/tmp;
if ( !-f $request_filename )
{
        proxy_pass ;
}
}

可以看到这个配置比404节约了不少代码，它是用!-f来判断请求的文件在文件系统上存不存在，不存在就proxy_pass到后端，返回同样是用proxy_store保存。

两种传统缓存都有着基本一样的优点和缺点：

缺点1：不支持带参数的动态链接，比如read.php?id=1，因为nginx只保存文件名，所以这个链接只在文件系统下保存为read.php，这样用户访问read.php?id=2时会返回不正确的结果。同时不支持这种形式的首页和二级目录，因为nginx非常老实，会将这样的请求照链接写入文件系统，而这个链接显然是一个目录，所以保存失败。这些情况都需要写rewrite才能正确保存。
缺点2：nginx内部没有缓存过期和清理的任何机制，这些缓存的文件会永久性地保存在机器上，如果要缓存的东西非常多，那就会撑暴整个硬盘空间。为此可以使用一个shell脚本定期清理，同时可以撰写php等动态程序来做实时更新。
缺点3：只能缓存200状态码，因此后端返回301/302/404等状态码都不会缓存，假如恰好有一个访问量很大的伪静态链接被删除，那就会不停穿透导致后端承载不小压力。
缺点4：nginx不会自动选择内存或硬盘作为存储介质，一切由配置决定，当然在当前的操作系统里都会有操作系统级的文件缓存机制，所以存在硬盘上也不需要过分担心大并发读取造成的io性能问题。

nginx传统缓存的缺点也是它和squid等缓存软件的不同之特色，所以也可看作其优点。在生产应用中它常常用作和squid的搭档，squid对于带?的链接往往无法阻挡，而nginx能将其访问拦住，例如：?和在squid上会被当做两个链接，所以会造成两次穿透；而nginx只会保存一次，无论链接变成还是，均不能透过nginx缓存，从而有效地保护了后端主机。

nginx会非常老实地将链接形式保存到文件系统中，这样对于一个链接，可以很方便地查阅它在缓存机器上的缓存状态和内容，也可以很方便地和别的文件管理器如rsync等配合使用，它完完全全就是一个文件系统结构。

这两种传统缓存都可以在linux下将文件保存到/dev/shm里，一般我也是这么做的，这样可以利用系统内存来做缓存，利用内存的话，清理过期内容速度就会快得多。使用/dev/shm/时除了要把tmp目录也指向到/dev/shm这个分区外，如果有大量小文件和目录，还要修改一下这个内存分区的inode数量和最大容量：

mount -o size=2500M -o nr_inodes=480000 -o noatime,nodiratime -o remount /dev/shm

上面的命令在一台有3G内存的机器上使用，因为/dev/shm默认最大内存是系统内存的一半就是1500M，这条命令将其调大成2500M，同时shm系统inode数量默认情况下可能是不够用的，但有趣的是它可以随意调节，这里调节为480000保守了点，但也基本够用了。

3、基于memcached的缓存

nginx对memcached有所支持，但是功能并不是特别之强，性能上还是非常之优秀。

location /mem/ {
    if ( $uri ~ "^/mem/([0-9A-Za-z_]*)$" )
    {
     set $memcached_key "$1";
     memcached_pass     192.168.1.2:11211;
    }
    expires 70;
}

这个配置会将指明到memcached的abc这个key去取数据。

nginx目前没有写入memcached的任何机制，所以要往memcached里写入数据得用后台的动态语言完成，可以利用404定向到后端去写入数据。

4、基于第三方插件ncache

ncache是新浪兄弟开发的一个不错的项目，它利用nginx和memcached实现了一部分类似squid缓存的功能，我并没有使用这个插件的经验，可以参考：

&nbsp;

5、nginx新开发的proxy_cache功能

从nginx-0.7.44版开始，nginx支持了类似squid较为正规的cache功能，目前还处于开发阶段，支持相当有限，这个缓存是把链接用md5编码hash后保存，所以它可以支持任意链接，同时也支持404/301/302这样的非200状态。

配置：

首先配置一个cache空间：

proxy_cache_path /path/to/cache levels=1:2 keys_zone=NAME:10m inactive=5m max_size=2m clean_time=1m;

#注意这个配置是在server标签外，levels指定该缓存空间有两层hash目录，第一层目录是1个字母，第二层为2个字母，保存的文件名就会类似/path/to/cache/c/29/b7f54b2df7773722d382f4809d65029c；keys_zone为这个空间起个名字，10m指空间大小为10MB；inactive的5m指缓存默认时长5分钟；max_size的2m是指单个文件超过2m的就不缓存；clean_time指定一分钟清理一次缓存。

location / {
    proxy_pass ;

    proxy_cache NAME;#使用NAME这个keys_zone

    proxy_cache_valid 200 302 1h;#200和302状态码保存1小时
    proxy_cache_valid 301 1d;#301状态码保存一天
    proxy_cache_valid any 1m;#其它的保存一分钟
}

ps：支持cache的0.7.44到0.7.51这几个版本的稳定性均有问题，访问有些链接会出现错误，所以这几个版本最好不要在生产环境中使用。nginx-0.7下目前所知较为稳定的版本是0.7.39。稳定版0.6.36版也是近期更新，如果在配置里没有使用到0.7的一些新标签新功能，也可以使用0.6.36版。

</div>
</div>
</div>
</div>

]]></content>
  </entry>
  
</feed>
